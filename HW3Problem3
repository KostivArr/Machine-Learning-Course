import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1/(1+np.exp(-z))

#define loss function for one piece of data
def loss_function(w,x,t):
    return ((1-t)*np.log(sigmoid(np.dot(w,x)))+t*np.log(1-sigmoid(np.dot(w,x))))

def gradient_function(w,x,t):
    return (1-sigmoid(np.dot(w,x))-t)*x

N=100  #Sample size
M=10   #Batch size
iterations=1000
rate=0.01   #learning rate

x_raw=np.random.rand(N,3)
x = np.hstack((np.ones((N, 1)), x_raw))
w=np.random.rand(x.shape[1])
t=np.random.randint(0,2,size=N)


for epoch in range(iterations):
    selected_indices=np.random.choice(N,M,replace=False)
    gradient_sum=np.sum([gradient_function(w,x[i],t[i]) for i in selected_indices],axis=0)
    w -= rate * gradient_sum
print(w)

predicted_raw = sigmoid(np.dot(x, w))
predicted=np.where(predicted_raw<0.5,0,1)
