import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1/(1+np.exp(-z))

#define loss function for one piece of data
def loss_function(w,x,t):
    return (t*np.log(sigmoid(np.dot(w,x)))+(1-t)*np.log(1-sigmoid(np.dot(w,x))))

def gradient_function(w,x,t):
    return (sigmoid(np.dot(w,x))-t)*x

N=100  #Sample size
M=10   #Batch size
iterations=1000
rate=0.01   #learning rate
w=np.random.rand(x.shape[1])

for epoch in range(iterations):
    selected_indices=np.random.choice(N,M,replace=False)
    gradient_sum=np.sum([gradient_function(w,x[i],t[i]) for i in selected_indices],axis=0)
    w -= rate * gradient_sum
print(w)

predicted_raw = sigmoid(np.dot(x, w))
predicted=np.where(predicted_raw<0.5,0,1)
