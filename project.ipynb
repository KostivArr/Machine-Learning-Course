{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ce7519-7135-41c7-b36e-34cd83a5eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# For word embeddings\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a43a9b-49f0-4146-84b5-41c162aee6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads all text files from a given directory and returns a list of their contents.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):  # Only process .txt files\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf8') as f:\n",
    "                    reviews.append(f.read().strip())\n",
    "    return reviews\n",
    "    \n",
    "# Directory paths (adjust these paths as needed)\n",
    "train_dir = 'train'  # Assumes the 'train' directory is in the current working directory\n",
    "test_dir = 'test'    # Assumes the 'test' directory is in the current working directory\n",
    "\n",
    "# Load negative and positive reviews from training data\n",
    "train_reviews_neg = load_reviews_from_directory(os.path.join(train_dir, 'neg'))\n",
    "train_reviews_pos = load_reviews_from_directory(os.path.join(train_dir, 'pos'))\n",
    "\n",
    "# Combine negative and positive training reviews and labels\n",
    "reviews_train = train_reviews_neg + train_reviews_pos\n",
    "train_labels = [0]*len(train_reviews_neg) + [1]*len(train_reviews_pos)  # 0: negative, 1: positive\n",
    "\n",
    "# Load negative and positive reviews from testing data\n",
    "test_reviews_neg = load_reviews_from_directory(os.path.join(test_dir, 'neg'))\n",
    "test_reviews_pos = load_reviews_from_directory(os.path.join(test_dir, 'pos'))\n",
    "\n",
    "# Combine negative and positive testing reviews and labels\n",
    "reviews_test = test_reviews_neg + test_reviews_pos\n",
    "test_labels = [0]*len(test_reviews_neg) + [1]*len(test_reviews_pos)  # 0: negative, 1: positive\n",
    "\n",
    "# Regular expressions for text cleaning\n",
    "REPLACE_NO_SPACE = re.compile(r\"[\\.;:!\\?',\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(r\"(<br\\s*/><br\\s*/>)|[-/]\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \"\"\"\n",
    "    Cleans the input text by:\n",
    "    - Converting to lowercase\n",
    "    - Removing certain punctuation marks\n",
    "    - Replacing some patterns with space\n",
    "    \"\"\"\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    return reviews\n",
    "\n",
    "# Clean training and testing data\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)\n",
    "\n",
    "# Import gensim library\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "embed_lookup = KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300-SLIM.bin', binary=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81c75097-7348-42f4-a029-7d19bb6b0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(embed_lookup, reviews):\n",
    "    \"\"\"\n",
    "    Tokenizes the reviews using the pre-trained Word2Vec model's vocabulary.\n",
    "    Words not in the vocabulary are mapped to index 0.\n",
    "    \"\"\"\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews:\n",
    "        tokens = []\n",
    "        for word in review.split():\n",
    "            if word in embed_lookup.key_to_index:\n",
    "                tokens.append(embed_lookup.key_to_index[word])\n",
    "            else:\n",
    "                tokens.append(0)  # Unknown words mapped to 0\n",
    "        tokenized_reviews.append(tokens)\n",
    "    return tokenized_reviews\n",
    "\n",
    "# Tokenize training and testing reviews\n",
    "tokenized_reviews_train = tokenize_reviews(embed_lookup, reviews_train_clean)\n",
    "tokenized_reviews_test = tokenize_reviews(embed_lookup, reviews_test_clean)\n",
    "\n",
    "# Remove zero-length reviews from training data\n",
    "non_zero_idx_train = [idx for idx, review in enumerate(tokenized_reviews_train) if len(review) != 0]\n",
    "tokenized_reviews_train = [tokenized_reviews_train[idx] for idx in non_zero_idx_train]\n",
    "train_labels = [train_labels[idx] for idx in non_zero_idx_train]\n",
    "\n",
    "# Remove zero-length reviews from testing data\n",
    "non_zero_idx_test = [idx for idx, review in enumerate(tokenized_reviews_test) if len(review) != 0]\n",
    "tokenized_reviews_test = [tokenized_reviews_test[idx] for idx in non_zero_idx_test]\n",
    "test_labels = [test_labels[idx] for idx in non_zero_idx_test]\n",
    "\n",
    "def pad_features(tokenized_reviews, seq_length):\n",
    "    \"\"\"\n",
    "    Return features of tokenized_reviews, where each review is padded with 0's \n",
    "    or truncated to the input seq_length.\n",
    "    \"\"\"\n",
    "    features = np.zeros((len(tokenized_reviews), seq_length), dtype=int)\n",
    "    \n",
    "    for i, review in enumerate(tokenized_reviews):\n",
    "        if len(review) <= seq_length:\n",
    "            features[i, -len(review):] = np.array(review)\n",
    "        else:\n",
    "            features[i, :] = np.array(review[:seq_length])\n",
    "    return features\n",
    "\n",
    "# Set sequence length (e.g., 200)\n",
    "seq_length = 200\n",
    "\n",
    "# Pad training and testing data\n",
    "features_train = pad_features(tokenized_reviews_train, seq_length)\n",
    "features_test = pad_features(tokenized_reviews_test, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21316cc7-68c1-48b6-b5d4-6754d33b3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(5000, 200) \n",
      "Test set: \t\t(25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(features_train) * split_frac)\n",
    "train_x, val_x = features_train[:split_idx], features_train[split_idx:]\n",
    "train_y, val_y = train_labels[:split_idx], train_labels[split_idx:]\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(features_test.shape))\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(features_test), torch.from_numpy(test_labels))\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03e203d5-3201-452a-b9a7-a4e986932bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model, output_size, num_filters=100, kernel_sizes=[3, 4, 5], \n",
    "                 freeze_embeddings=True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        # 1. Embedding layer\n",
    "        # Get embeddings from the pre-trained model\n",
    "        embedding_dim = embed_model.vector_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embed_model.vectors), freeze=freeze_embeddings\n",
    "        )\n",
    "        \n",
    "        # 2. Convolutional layers\n",
    "        self.convs_1d = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(k, embedding_dim), padding=(k-2, 0)) \n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        # 3. Fully-connected layer\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size)\n",
    "\n",
    "        # 4. Dropout and activation\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional layer with ReLU activation and max pooling.\n",
    "        \"\"\"\n",
    "        x = F.relu(conv(x)).squeeze(3)  # Remove last dimension\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)  # Max pooling\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how the model processes the input data.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # Embedding layer\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for convolutional layer\n",
    "\n",
    "        # Convolutional and pooling layers\n",
    "        x = [self.conv_and_pool(x, conv) for conv in self.convs_1d]\n",
    "\n",
    "        # Concatenate outputs and apply dropout\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Fully-connected layer and sigmoid activation\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "560cea39-d6d4-4a98-bea2-1008405f9530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n",
      "SentimentCNN(\n",
      "  (embedding): Embedding(299567, 300)\n",
      "  (convs_1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "output_size = 1  # Binary classification\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "dropout_prob = 0.5\n",
    "freeze_embeddings = True  # Set to False to fine-tune embeddings\n",
    "\n",
    "net = SentimentCNN(\n",
    "    embed_model=embed_lookup, \n",
    "    output_size=output_size, \n",
    "    num_filters=num_filters, \n",
    "    kernel_sizes=kernel_sizes, \n",
    "    freeze_embeddings=freeze_embeddings, \n",
    "    drop_prob=dropout_prob\n",
    ")\n",
    "\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a421a13-8c51-4299-889e-3c6401e39ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimization functions\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "def train(net, train_loader, valid_loader, epochs, print_every=100):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "    \"\"\"\n",
    "    # Move model to GPU if available\n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    \n",
    "    net.train()\n",
    "    counter = 0  # For printing\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if train_on_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # Get output from the model\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Calculate loss and perform backpropagation\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss statistics\n",
    "            if counter % print_every == 0:\n",
    "                net.eval()\n",
    "                val_losses = []\n",
    "                for val_inputs, val_labels in valid_loader:\n",
    "                    if train_on_gpu:\n",
    "                        val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()\n",
    "                    val_outputs = net(val_inputs)\n",
    "                    val_loss = criterion(val_outputs.squeeze(), val_labels.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                net.train()\n",
    "                print(f\"Epoch: {e+1}/{epochs}, Step: {counter}, \"\n",
    "                      f\"Loss: {loss.item():.6f}, Val Loss: {np.mean(val_losses):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f2eef75-c06c-4087-8418-696cf41d6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2, Step: 100, Loss: 0.525663, Val Loss: 0.850869\n",
      "Epoch: 1/2, Step: 200, Loss: 0.444582, Val Loss: 0.919653\n",
      "Epoch: 1/2, Step: 300, Loss: 0.442624, Val Loss: 0.690815\n",
      "Epoch: 1/2, Step: 400, Loss: 0.367176, Val Loss: 0.428920\n",
      "Epoch: 2/2, Step: 500, Loss: 0.292224, Val Loss: 0.465606\n",
      "Epoch: 2/2, Step: 600, Loss: 0.347272, Val Loss: 0.440414\n",
      "Epoch: 2/2, Step: 700, Loss: 0.365542, Val Loss: 0.553944\n",
      "Epoch: 2/2, Step: 800, Loss: 0.272099, Val Loss: 0.519187\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 2  # Adjust as needed\n",
    "print_every = 100\n",
    "\n",
    "# Train the model\n",
    "train(net, train_loader, valid_loader, epochs, print_every)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fec926a-83d7-42c2-94eb-553bb870dd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.349\n",
      "Test Accuracy: 0.846\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "def test(net, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test data.\n",
    "    \"\"\"\n",
    "    test_losses = []\n",
    "    num_correct = 0\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if train_on_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            test_loss = criterion(outputs.squeeze(), labels.float())\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            # Convert output probabilities to predicted class (0 or 1)\n",
    "            preds = torch.round(outputs.squeeze())\n",
    "\n",
    "            # Compare predictions to true labels\n",
    "            correct_tensor = preds.eq(labels.float())\n",
    "            correct = np.squeeze(correct_tensor.cpu().numpy()) if not train_on_gpu else np.squeeze(correct_tensor.numpy())\n",
    "            num_correct += np.sum(correct)\n",
    "\n",
    "    print(f\"Test Loss: {np.mean(test_losses):.3f}\")\n",
    "    print(f\"Test Accuracy: {num_correct / len(test_loader.dataset):.3f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "test(net, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47aae707-59d1-41c8-a9f5-a59268fc8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_review(embed_lookup, review):\n",
    "    \"\"\"\n",
    "    Tokenizes a single review using the pre-trained Word2Vec model.\n",
    "    \"\"\"\n",
    "    review = review.lower()\n",
    "    review = ''.join([c for c in review if c not in punctuation])\n",
    "    tokens = []\n",
    "    for word in review.split():\n",
    "        if word in embed_lookup.key_to_index:\n",
    "            tokens.append(embed_lookup.key_to_index[word])\n",
    "        else:\n",
    "            tokens.append(0)  # Unknown words mapped to 0\n",
    "    return tokens\n",
    "\n",
    "def predict(net, embed_lookup, review, seq_length=200):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a single review.\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    tokens = tokenize_review(embed_lookup, review)\n",
    "    features = pad_features([tokens], seq_length)\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "\n",
    "    if train_on_gpu:\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = net(feature_tensor)\n",
    "\n",
    "    # Convert output probability to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    print(f\"Prediction value (before rounding): {output.item():.6f}\")\n",
    "\n",
    "    if pred.item() == 1:\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92dd2932-1a9f-454b-854e-8e01b74f26db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.846\n",
      "Test Precision: 0.906\n",
      "Test Recall: 0.773\n",
      "Test F1 Score: 0.834\n"
     ]
    }
   ],
   "source": [
    "# Create Tensor dataset for test data\n",
    "test_data = TensorDataset(torch.from_numpy(features_test), torch.from_numpy(test_labels))\n",
    "\n",
    "# Dataloader for test data\n",
    "batch_size = 50  # Use the same batch size as during training\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(net, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test data and compute evaluation metrics.\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if train_on_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "            outputs = net(inputs)\n",
    "            preds = torch.round(outputs.squeeze())  # Get the predicted class (0 or 1)\n",
    "            preds = preds.cpu().numpy() if train_on_gpu else preds.numpy()\n",
    "            labels = labels.numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Test Precision: {precision:.3f}\")\n",
    "    print(f\"Test Recall: {recall:.3f}\")\n",
    "    print(f\"Test F1 Score: {f1:.3f}\")\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluate_model(net, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7fcf9d8-ba98-4895-9385-3a2fc52852d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.92      0.86     12500\n",
      "    Positive       0.91      0.77      0.83     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def classification_report_model(net, test_loader):\n",
    "    \"\"\"\n",
    "    Generate a classification report for the test data.\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if train_on_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "            outputs = net(inputs)\n",
    "            preds = torch.round(outputs.squeeze())  # Get the predicted class (0 or 1)\n",
    "            preds = preds.cpu().numpy() if train_on_gpu else preds.numpy()\n",
    "            labels = labels.numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Generate classification report\n",
    "    target_names = ['Negative', 'Positive']\n",
    "    report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "    print(report)\n",
    "\n",
    "# Call the classification report function\n",
    "classification_report_model(net, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
